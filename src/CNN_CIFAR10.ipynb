{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXqCaVi+/yaETN6jg5xnAb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rainwoorimforest/pytorch-study/blob/main/src/CNN_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy6BnLCcpVPY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(DEVICE, torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVRON6MGpdQy",
        "outputId": "d2cfd75f-2bea-41df-9c44-f2bb1e0124e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu 2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root=\"data/CIFAR10_DATA/\", train = True, download=True,\n",
        "                                 transform=transforms.ToTensor())\n",
        "test_dataset = datasets.CIFAR10(root=\"data/CIFAR10_DATA/\", train = False, download=True,\n",
        "                                 transform=transforms.ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pimjPpWLpjEd",
        "outputId": "a6bbdaaf-4958-4098-8532-9d7a71060c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 50.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset)) #batch_size를 알기 위해서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2fo_pudp9ZD",
        "outputId": "9d5b0a6b-9019-4e0d-99f2-b8d0ab7cc60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_size = int(len(train_dataset) * 0.85)\n",
        "valid_dataset_size = len(train_dataset) - train_dataset_size\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [train_dataset_size, valid_dataset_size])"
      ],
      "metadata": {
        "id": "CCkGyoSaqIW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(valid_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obqaM0CHqWpY",
        "outputId": "c39abb76-429b-4745-bde6-9acb4dcffb6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42500\n",
            "7500\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE  = 64\n",
        "\n",
        "train_dataset_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
        "valid_dataset_loader = DataLoader(dataset = valid_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
        "test_dataset_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "kuuumGtmqZSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(8 * 8 * 64, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "        self.dropout25 = nn.Dropout(p=0.25)\n",
        "        self.dropout50 = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        data = self.conv1(data)\n",
        "        data = torch.relu(data)\n",
        "        data = self.pooling(data)\n",
        "        data = self.dropout25(data)\n",
        "\n",
        "        data = self.conv2(data)\n",
        "        data = torch.relu(data)\n",
        "        data = self.pooling(data)\n",
        "        data = self.dropout25(data)\n",
        "\n",
        "        data = data.view(-1, 8 * 8 * 64)\n",
        "\n",
        "        data = self.fc1(data)\n",
        "        data = torch.relu(data)\n",
        "        data = self.dropout50(data)\n",
        "\n",
        "        logits = self.fc2(data)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "QLmh7mLhqfaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyCNNModel().to(DEVICE)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "NeGwNfKorXfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "collapsed": true,
        "id": "3B7kppiIrsfv",
        "outputId": "28d5f213-9e92-4b01-b6c1-d8a2ffa873e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-908604d4d204>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(dataloader, model, loss_function, optimizer):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss_sum = train_correct = train_total = 0\n",
        "\n",
        "    total_train_batch = len(dataloader)\n",
        "    print(len(dataloader))\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "\n",
        "        x_train = images.to(DEVICE)\n",
        "        y_train = labels.to(DEVICE)\n",
        "\n",
        "        outputs = model(x_train)\n",
        "        loss = loss_function(outputs, y_train)\n",
        "\n",
        "        optimizer.zero_grad() # 이전 배치에서 계산된 기울기(gradient)를 초기화\n",
        "        loss.backward() # 역전파 w 등 하이퍼파라미터 기울기 누적\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_sum += loss.item()\n",
        "\n",
        "        train_total += y_train.size(0)\n",
        "        train_correct += ((torch.argmax(outputs, 1)==y_train)).sum().item()\n",
        "\n",
        "    train_avg_loss = train_loss_sum / total_train_batch\n",
        "    train_avg_accuracy = 100*train_correct / train_total\n",
        "\n",
        "    return (train_avg_loss, train_avg_accuracy)"
      ],
      "metadata": {
        "id": "gIQKf1Dqrd5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluate(dataloader, model, loss_function, optimizer):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        val_loss_sum = val_correct = val_total = 0\n",
        "\n",
        "        total_val_batch = len(dataloader)\n",
        "\n",
        "        for images, labels in dataloader:\n",
        "\n",
        "            x_val = images.to(DEVICE)\n",
        "            y_val = labels.to(DEVICE)\n",
        "\n",
        "            outputs = model(x_val)\n",
        "            loss = loss_function(outputs, y_val)\n",
        "\n",
        "            val_loss_sum += loss.item()\n",
        "\n",
        "            val_total += y_val.size(0)\n",
        "            val_correct += ((torch.argmax(outputs, 1)==y_val)).sum().item()\n",
        "\n",
        "        val_avg_loss = val_loss_sum / total_val_batch\n",
        "        val_avg_accuracy = 100*val_correct / val_total\n",
        "\n",
        "    return (val_avg_loss, val_avg_accuracy)"
      ],
      "metadata": {
        "id": "zhIfanUQrpAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def model_test(dataloader, model):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_loss_sum = test_correct = test_total = 0\n",
        "\n",
        "        total_test_batch = len(dataloader)\n",
        "\n",
        "        for images, labels in dataloader:\n",
        "\n",
        "            x_test = images.to(DEVICE)\n",
        "            y_test = labels.to(DEVICE)\n",
        "\n",
        "            outputs = model(x_test)\n",
        "            loss = loss_function(outputs, y_test)\n",
        "\n",
        "            test_loss_sum += loss.item()\n",
        "\n",
        "            test_total += y_test.size(0)\n",
        "            test_correct += ((torch.argmax(outputs, 1)==y_test)).sum().item()\n",
        "\n",
        "        test_avg_loss = test_loss_sum / total_test_batch\n",
        "        test_avg_accuracy = 100*test_correct / test_total\n",
        "\n",
        "        print('accuracy:', test_avg_accuracy)\n",
        "        print('loss:', test_avg_loss)"
      ],
      "metadata": {
        "id": "i4d4u3wotmH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "train_loss_list = []\n",
        "train_accuracy_list = []\n",
        "\n",
        "val_loss_list = []\n",
        "val_accuracy_list = []\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "   #==============  model train  ================\n",
        "    train_avg_loss, train_avg_accuracy = model_train(train_dataset_loader, model, loss_function, optimizer)\n",
        "\n",
        "    train_loss_list.append(train_avg_loss)\n",
        "    train_accuracy_list.append(train_avg_accuracy)\n",
        "    #=============================================\n",
        "\n",
        "    #============  model evaluation  ==============\n",
        "    val_avg_loss, val_avg_accuracy = model_evaluate(valid_dataset_loader, model, loss_function, optimizer)\n",
        "\n",
        "    val_loss_list.append(val_avg_loss)\n",
        "    val_accuracy_list.append(val_avg_accuracy)\n",
        "    #============  model evaluation  ==============\n",
        "\n",
        "    print('epoch:', '%02d' % (epoch + 1),\n",
        "          'train loss =', '{:.3f}'.format(train_avg_loss), 'train acc =', '{:.3f}'.format(train_avg_accuracy),\n",
        "          'val loss =', '{:.3f}'.format(val_avg_loss), 'val acc =', '{:.3f}'.format(val_avg_accuracy))\n",
        "\n",
        "end_time = datetime.now()\n",
        "\n",
        "print('elapsed time => ', end_time-start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwPepBxqwHu2",
        "outputId": "77511b8c-e37a-4bde-a122-f3d15d60f9dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "665\n",
            "epoch: 01 train loss = 1.441 train acc = 47.866 val loss = 1.285 val acc = 53.253\n",
            "665\n",
            "epoch: 02 train loss = 1.326 train acc = 52.266 val loss = 1.184 val acc = 57.400\n",
            "665\n",
            "epoch: 03 train loss = 1.256 train acc = 54.741 val loss = 1.107 val acc = 61.093\n",
            "665\n",
            "epoch: 04 train loss = 1.199 train acc = 56.885 val loss = 1.056 val acc = 62.840\n",
            "665\n",
            "epoch: 05 train loss = 1.149 train acc = 59.024 val loss = 0.976 val acc = 65.227\n",
            "665\n",
            "epoch: 06 train loss = 1.111 train acc = 59.984 val loss = 0.957 val acc = 66.067\n",
            "665\n",
            "epoch: 07 train loss = 1.082 train acc = 61.358 val loss = 0.976 val acc = 65.880\n",
            "665\n",
            "epoch: 08 train loss = 1.056 train acc = 62.626 val loss = 0.931 val acc = 67.333\n",
            "665\n",
            "epoch: 09 train loss = 1.032 train acc = 63.351 val loss = 0.902 val acc = 68.573\n",
            "665\n",
            "epoch: 10 train loss = 1.010 train acc = 63.711 val loss = 0.893 val acc = 68.280\n",
            "665\n",
            "epoch: 11 train loss = 0.994 train acc = 64.546 val loss = 0.884 val acc = 69.173\n",
            "665\n",
            "epoch: 12 train loss = 0.975 train acc = 65.492 val loss = 0.874 val acc = 69.147\n",
            "665\n",
            "epoch: 13 train loss = 0.959 train acc = 65.685 val loss = 0.868 val acc = 69.853\n",
            "665\n",
            "epoch: 14 train loss = 0.946 train acc = 66.320 val loss = 0.911 val acc = 68.000\n",
            "665\n",
            "epoch: 15 train loss = 0.932 train acc = 66.687 val loss = 0.848 val acc = 70.933\n",
            "665\n",
            "epoch: 16 train loss = 0.914 train acc = 67.402 val loss = 0.881 val acc = 68.827\n",
            "665\n",
            "epoch: 17 train loss = 0.903 train acc = 67.779 val loss = 0.890 val acc = 69.667\n",
            "665\n",
            "epoch: 18 train loss = 0.890 train acc = 68.602 val loss = 0.842 val acc = 70.400\n",
            "665\n",
            "epoch: 19 train loss = 0.876 train acc = 68.407 val loss = 0.852 val acc = 70.427\n",
            "665\n",
            "epoch: 20 train loss = 0.876 train acc = 68.365 val loss = 0.827 val acc = 71.840\n",
            "665\n",
            "epoch: 21 train loss = 0.858 train acc = 69.195 val loss = 0.825 val acc = 71.120\n",
            "665\n",
            "epoch: 22 train loss = 0.853 train acc = 69.506 val loss = 0.861 val acc = 70.093\n",
            "665\n",
            "epoch: 23 train loss = 0.846 train acc = 69.511 val loss = 0.820 val acc = 71.267\n",
            "665\n",
            "epoch: 24 train loss = 0.833 train acc = 70.205 val loss = 0.818 val acc = 71.480\n",
            "665\n",
            "epoch: 25 train loss = 0.831 train acc = 70.181 val loss = 0.814 val acc = 72.027\n",
            "665\n",
            "epoch: 26 train loss = 0.817 train acc = 70.664 val loss = 0.802 val acc = 72.267\n",
            "665\n",
            "epoch: 27 train loss = 0.816 train acc = 70.776 val loss = 0.804 val acc = 71.933\n",
            "665\n",
            "epoch: 28 train loss = 0.806 train acc = 71.012 val loss = 0.810 val acc = 71.680\n",
            "665\n",
            "epoch: 29 train loss = 0.800 train acc = 71.061 val loss = 0.804 val acc = 72.187\n",
            "665\n",
            "epoch: 30 train loss = 0.799 train acc = 71.395 val loss = 0.809 val acc = 72.147\n",
            "665\n",
            "epoch: 31 train loss = 0.791 train acc = 71.466 val loss = 0.794 val acc = 72.293\n",
            "665\n",
            "epoch: 32 train loss = 0.777 train acc = 71.925 val loss = 0.808 val acc = 71.960\n",
            "665\n",
            "epoch: 33 train loss = 0.774 train acc = 72.132 val loss = 0.833 val acc = 71.493\n",
            "665\n",
            "epoch: 34 train loss = 0.774 train acc = 71.922 val loss = 0.828 val acc = 71.893\n",
            "665\n",
            "epoch: 35 train loss = 0.769 train acc = 72.049 val loss = 0.799 val acc = 72.320\n",
            "665\n",
            "epoch: 36 train loss = 0.763 train acc = 72.466 val loss = 0.787 val acc = 72.960\n",
            "665\n",
            "epoch: 37 train loss = 0.754 train acc = 72.612 val loss = 0.803 val acc = 72.560\n",
            "665\n",
            "epoch: 38 train loss = 0.751 train acc = 72.751 val loss = 0.802 val acc = 72.787\n",
            "665\n",
            "epoch: 39 train loss = 0.751 train acc = 72.539 val loss = 0.803 val acc = 72.253\n",
            "665\n",
            "epoch: 40 train loss = 0.752 train acc = 72.840 val loss = 0.821 val acc = 71.787\n",
            "665\n",
            "epoch: 41 train loss = 0.739 train acc = 73.327 val loss = 0.790 val acc = 72.707\n",
            "665\n",
            "epoch: 42 train loss = 0.739 train acc = 73.118 val loss = 0.785 val acc = 73.027\n",
            "665\n",
            "epoch: 43 train loss = 0.735 train acc = 73.249 val loss = 0.793 val acc = 72.453\n",
            "665\n",
            "epoch: 44 train loss = 0.728 train acc = 73.384 val loss = 0.814 val acc = 71.840\n",
            "665\n",
            "epoch: 45 train loss = 0.724 train acc = 73.725 val loss = 0.799 val acc = 72.560\n",
            "665\n",
            "epoch: 46 train loss = 0.720 train acc = 73.906 val loss = 0.806 val acc = 72.813\n",
            "665\n",
            "epoch: 47 train loss = 0.720 train acc = 73.769 val loss = 0.785 val acc = 73.000\n",
            "665\n",
            "epoch: 48 train loss = 0.713 train acc = 73.960 val loss = 0.807 val acc = 72.267\n",
            "665\n",
            "epoch: 49 train loss = 0.720 train acc = 73.633 val loss = 0.796 val acc = 73.320\n",
            "665\n",
            "epoch: 50 train loss = 0.709 train acc = 74.052 val loss = 0.800 val acc = 72.613\n",
            "665\n",
            "epoch: 51 train loss = 0.711 train acc = 74.132 val loss = 0.792 val acc = 73.133\n",
            "665\n",
            "epoch: 52 train loss = 0.708 train acc = 74.179 val loss = 0.783 val acc = 73.053\n",
            "665\n",
            "epoch: 53 train loss = 0.698 train acc = 74.407 val loss = 0.783 val acc = 73.227\n",
            "665\n",
            "epoch: 54 train loss = 0.692 train acc = 74.953 val loss = 0.792 val acc = 73.587\n",
            "665\n",
            "epoch: 55 train loss = 0.702 train acc = 74.638 val loss = 0.782 val acc = 73.427\n",
            "665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Loss Trend')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "plt.plot(train_loss_list, label='train')\n",
        "plt.plot(val_loss_list, label='validation')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('Accuracy Trend')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.grid()\n",
        "plt.plot(train_accuracy_list, label='train')\n",
        "plt.plot(val_accuracy_list, label='validation')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uUoe4pEGy2U9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}